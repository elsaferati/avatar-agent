<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PrimEx AI Presenter V2</title>
    <style>
        /* --- STYLING --- */
        body { 
            background: #111; color: #fff; font-family: 'Segoe UI', sans-serif; 
            display: flex; height: 100vh; margin: 0; overflow: hidden; 
        }
        .col { padding: 25px; display: flex; flex-direction: column; gap: 20px; }
        .left { flex: 2; border-right: 1px solid #333; }
        .right { flex: 1; background: #1a1a1a; display: flex; flex-direction: column; position: relative; }
       
        /* PDF STAGE */
        #pdf-stage { 
            flex: 1; background: #000; display: flex; justify-content: center; 
            align-items: center; border: 1px solid #333; border-radius: 12px; overflow: hidden; 
        }
        canvas { max-width: 100%; max-height: 100%; object-fit: contain;}

        /* NEW: CHAT HISTORY (Replaces Caption Box) */
        #chat-history {
            background: #0e0e0e; border: 1px solid #333; border-radius: 8px; padding: 15px;
            height: 150px; overflow-y: auto; font-size: 14px; line-height: 1.5;
            display: flex; flex-direction: column; gap: 10px;
        }
        .msg-row { display: flex; gap: 8px; }
        .msg-role { font-weight: bold; min-width: 40px; }
        .msg-text { color: #ccc; }
        .role-elsa { color: #007bff; }
        .role-user { color: #28a745; }

        /* AVATAR STAGE */
        #avatar-container { 
            height: 300px; width: 100%; background: #000; position: relative; 
            overflow: hidden; border-radius: 12px; border: 1px solid #333;
            transition: all 0.3s ease;
        }
        .brand-badge {
            position: absolute; top: 12px; left: 12px; z-index: 20;
            display: inline-flex; align-items: center; justify-content: center;
            padding: 4px; border-radius: 999px; border: 1px solid #333;
            background: rgba(255, 255, 255);
            box-shadow: 0 6px 20px rgba(0, 0, 0, 0.35);
        }
        .brand-badge img { width: 34px; height: 34px; object-fit: contain; }

        video { width: 100%; height: 100%; object-fit: cover; }
        
        /* OVERLAY */
        #connect-overlay {
            position: absolute; inset: 0; background: rgba(0,0,0,0.85); 
            display: flex; justify-content: center; align-items: center; 
            flex-direction: column; gap: 15px; z-index: 10;
        }

        /* CONTROLS */
        .controls { margin-top: auto; display: flex; flex-direction: column; gap: 10px; }
        .btn-group { display: flex; gap: 10px; }
        
        button { 
            padding: 12px; cursor: pointer; background: #333; color: white; 
            border: none; border-radius: 6px; font-weight: 600; font-size: 13px; transition: 0.2s;
        }
        button:hover { background: #444; }
        button:disabled { opacity: 0.5; cursor: not-allowed; }
        
        .btn-primary { background: #007bff; }
        .btn-primary:hover { background: #0056b3; }
        .btn-danger { background: #dc3545; }
        .btn-danger:hover { background: #a71d2a; }

        .input-group { display: flex; gap: 10px; }
        textarea { 
            flex: 1; padding: 12px; background: #222; color: white; 
            border: 1px solid #444; resize: none; border-radius: 8px; 
            font-family: inherit; outline: none;
        }

        #mic-btn.listening { background: #ff4d4d; animation: pulse 1.5s infinite; }
        @keyframes pulse { 0% { box-shadow: 0 0 0 0 rgba(255, 77, 77, 0.7); } 100% { box-shadow: 0 0 0 0 rgba(255, 77, 77, 0); } }
        
        .speaking-mode { border-color: #007bff !important; box-shadow: 0 0 20px rgba(0, 123, 255, 0.3); }
    </style>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js"></script>
    <script>pdfjsLib.GlobalWorkerOptions.workerSrc = "https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.worker.min.js";</script>
</head>
<body>

    <!-- LEFT: SLIDES -->
    <div class="col left">
        <div style="display:flex; justify-content: space-between; align-items: center;">
            <h3 style="margin:0">üìä Presentation Deck</h3>
            <input type="file" id="pdf-input" accept="application/pdf">
        </div>
        <div id="pdf-stage"><canvas id="pdf-canvas"></canvas></div>
    </div>
 <canvas id="hidden-canvas" style="display: none;"></canvas>
    <!-- RIGHT: AVATAR -->
    <div class="col right">
        <div id="avatar-container">
            <video id="simli-video" autoplay playsinline></video>
            <audio id="simli-audio" autoplay></audio>
            
            <div class="brand-badge">
                <img src="primex%20logo.png" alt="PrimEx logo">
            </div>
            
            <div id="connect-overlay">
                <h2>AI Presenter V2</h2>
                <button id="connect-btn" class="btn-primary">üîå Connect Elsa</button>
            </div>
        </div>

        <div style="text-align:center; font-size:12px; color:#666;" id="status-text">Status: Offline</div>

        <!-- NEW CHAT HISTORY -->
        <div id="chat-history">
            <div style="color:#555; font-style:italic;">Transcript will appear here...</div>
        </div>

        <div class="controls">
            <!-- Navigation Controls -->
            <div class="btn-group">
                <button id="prev-btn" disabled>‚èÆ Prev</button>
                <button id="start-btn" class="btn-primary" style="flex: 1;" disabled>‚ñ∂ Start</button>
                <button id="stop-btn" class="btn-danger" style="width: 60px;" disabled>‚èπ</button>
                <button id="next-btn" disabled>Next ‚è≠</button>
            </div>
            
            <!-- Input Area -->
            <div class="input-group">
                <textarea id="q-input" rows="1" placeholder="Ask a question..."></textarea>
                <button id="mic-btn" title="Speak">üéôÔ∏è</button>
            </div>
            <button id="send-btn" disabled>Send Message</button>
        </div>
    </div>
 
    <!-- LOGIC -->
   <!-- LOGIC -->
    <script type="module">
        import { SimliClient } from 'https://esm.sh/simli-client@latest';

        const BASE_URL = 'http://127.0.0.1:3000';
        
        // --- GLOBAL STATE ---
        let pdfDoc = null;
        let currentSlide = 1;      
        let isPresenting = false;  
        let isInterrupted = false; 
        
        // VAD (Voice Activity Detection) State
        let audioContext = null;
        let analyser = null;
        let microphoneStream = null;
        let dataArray = null;
        let isMonitoringMic = false;
        // TWEAK THIS: 0.0 to 1.0. Higher = you have to shout. Lower = sensitive.
        const INTERRUPTION_THRESHOLD = 0.25; 

        // State variables
        let slidesSinceLastAsk = 0;
        let recentUserQuestion = false;

        const simli = new SimliClient();
        
        // Voice Recognition
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        const recognition = new SpeechRecognition();
        recognition.continuous = false; 
        recognition.lang = 'en-US';

        // DOM Elements
        const videoEl = document.getElementById('simli-video');
        const audioEl = document.getElementById('simli-audio');
        const statusEl = document.getElementById('status-text');
        const chatLog = document.getElementById('chat-history');
        const avatarContainer = document.getElementById('avatar-container');
        const startBtn = document.getElementById('start-btn');
        const stopBtn = document.getElementById('stop-btn');
        const micBtn = document.getElementById('mic-btn');

        // --- 1. CONNECT & SETUP MIC MONITORING ---
        document.getElementById('connect-btn').onclick = async () => {
            const btn = document.getElementById('connect-btn');
            btn.textContent = "Connecting...";
            try {
                // Initialize Audio Context for Simli AND for VAD
                audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
                await audioContext.resume();

                const cfgReq = await fetch(`${BASE_URL}/simli-config`);
                const config = await cfgReq.json();
                
                simli.Initialize({ apiKey: config.apiKey, faceID: config.faceID, handleSilence: true, videoRef: videoEl, audioRef: audioEl });
                await simli.start();
                
                // --- START VAD (Listen for interruption) ---
                await setupVAD();

                statusEl.textContent = "Status: Online (Listening for interruption)";
                document.getElementById('connect-overlay').style.display = 'none';
                if(pdfDoc) enableControls(true);
                document.getElementById('send-btn').disabled = false;
            } catch (err) {
                alert("Error: " + err.message);
                btn.textContent = "Retry";
            }
        };

        function enableControls(enabled) {
            startBtn.disabled = !enabled;
            // prevBtn.disabled = !enabled; // simplified
            // nextBtn.disabled = !enabled;
        }

        // --- 2. VAD LOGIC (The "Interruption" Ear) ---
        async function setupVAD() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                microphoneStream = audioContext.createMediaStreamSource(stream);
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 512;
                microphoneStream.connect(analyser);
                dataArray = new Uint8Array(analyser.frequencyBinCount);
                
                isMonitoringMic = true;
                monitorVolume();
            } catch (e) {
                console.error("Mic access denied for VAD:", e);
            }
        }

        function monitorVolume() {
            if (!isMonitoringMic) return;
            
            requestAnimationFrame(monitorVolume);
            analyser.getByteFrequencyData(dataArray);

            // Calculate average volume
            let sum = 0;
            for(let i = 0; i < dataArray.length; i++) sum += dataArray[i];
            const average = sum / dataArray.length;
            const volume = average / 255; // Normalize 0.0 to 1.0

            // VISUAL DEBUG: Uncomment to see volume in console
            // if (volume > 0.05) console.log("Vol:", volume.toFixed(2));

            // INTERRUPTION TRIGGER
            // Only interrupt if:
            // 1. We are currently presenting (speaking)
            // 2. We are NOT already interrupted
            // 3. Volume is LOUD enough (User is talking)
            if (isPresenting && !isInterrupted && volume > INTERRUPTION_THRESHOLD) {
                console.log("üî¥ INTERRUPTION DETECTED! Volume:", volume);
                interruptAndListen();
            }
        }

        function interruptAndListen() {
            // 1. Stop the loop logic
            isInterrupted = true; 
            
            // 2. Kill the Audio/Video immediately
            simli.ClearBuffer(); 
            
            // 3. Update UI
            statusEl.textContent = "Status: I heard you! Listening...";
            micBtn.classList.add('listening');

            // 4. Start actual speech recognition to get the text
            try { 
                recognition.start(); 
            } catch(e) { 
                // It might already be started, that's fine
            }
        }

        // --- 3. INPUT HANDLING ---

        // A. Shared Processor
        async function processUserInput(userText) {
            if (!userText.trim()) return;

            try { recognition.stop(); } catch(e){}
            micBtn.classList.remove('listening');

            addLog("User", userText);
            document.getElementById('q-input').value = ""; 

            statusEl.textContent = "Status: Thinking...";
            
            try {
                const checkRes = await fetch(`${BASE_URL}/agent/speak`, {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ text: userText, type: 'DECIDE_NEXT_MOVE' })
                });
                const decision = await checkRes.json();

                if (decision.action === "RESUME") {
                    
                    if (recentUserQuestion) {
                        addLog("System", "Generating Recap...");
                        const context = await getPageText(currentSlide);
                        const recapRes = await fetch(`${BASE_URL}/agent/speak`, {
                            method: 'POST',
                            headers: {'Content-Type': 'application/json'},
                            body: JSON.stringify({ text: context, type: 'RECAP', slideIndex: currentSlide })
                        });
                        const recapData = await recapRes.json();
                        addLog("Elsa", recapData.text);
                        await playAudioData(recapData.audio);
                    }

                    // RESUME
                    addLog("System", "Resuming...");
                    statusEl.textContent = "Status: Resuming...";
                    isInterrupted = false; 
                    recentUserQuestion = false;
                    
                    // Force restart loop if it fully stopped
                    if (!isPresenting) startBtn.click();

                } else {
                    recentUserQuestion = true;
                    await handleQuestionLoop(userText);
                }
            } catch (err) {
                console.error(err);
                statusEl.textContent = "Error processing input";
            }
        }

        // B. Handle Question Loop
        async function handleQuestionLoop(questionText) {
            avatarContainer.classList.add('speaking-mode');
            const context = await getPageText(currentSlide);
            
            const res = await fetch(`${BASE_URL}/agent/speak`, {
                method: 'POST',
                headers: {'Content-Type': 'application/json'},
                body: JSON.stringify({ text: questionText, type: 'ANSWER', context })
            });
            const data = await res.json();
            
            addLog("Elsa", data.text);
            await playAudioData(data.audio);
            avatarContainer.classList.remove('speaking-mode');

            // Wait for user again (Manual or Auto based on silence)
            // We set isInterrupted = true so the presentation doesn't auto-resume.
            // The VAD will pick up the next "Resume" or Question command.
            statusEl.textContent = "Status: Waiting for you...";
            isInterrupted = true; 
        }

        recognition.onresult = (e) => {
            const transcript = e.results[0][0].transcript;
            document.getElementById('q-input').value = transcript;
            processUserInput(transcript); 
        };

        // --- 4. PRESENTATION LOOP (With Hidden Canvas) ---
        startBtn.onclick = async () => {
            if(!pdfDoc) return alert("Load PDF first");
            
            isPresenting = true;
            isInterrupted = false;
            slidesSinceLastAsk = 0;
            recentUserQuestion = false;
            
            startBtn.disabled = true;
            stopBtn.disabled = false;
            startBtn.textContent = "Presenting...";

            const totalSlides = pdfDoc.numPages;
            if (currentSlide > totalSlides) currentSlide = 1;

            if (currentSlide === 1) await renderPage(1);

            while (isPresenting && currentSlide <= totalSlides) {
                
                // If we were interrupted by voice, wait here until resolved
                while(isInterrupted && isPresenting) {
                    await new Promise(r => setTimeout(r, 500));
                }
                if (!isPresenting) break;

                statusEl.textContent = `Preparing Slide ${currentSlide}...`;

                // A. Background Gen
                await renderHiddenPage(currentSlide);
                const imageBase64 = document.getElementById('hidden-canvas').toDataURL('image/jpeg', 0.8);
                const text = await getPageText(currentSlide);
                
                // Check interrupt again before fetch
                if (isInterrupted) continue; 

                addLog("System", `Slide ${currentSlide}: Thinking...`);
                
                try {
                    const res = await fetch(`${BASE_URL}/agent/speak`, {
                        method: 'POST',
                        headers: {'Content-Type': 'application/json'},
                        body: JSON.stringify({ text, type: 'PRESENT', slideIndex: currentSlide, totalSlides, image: imageBase64 })
                    });
                    
                    const data = await res.json();
                    
                    // Check interrupt again before Playing
                    if (isInterrupted) continue;

                    if (data.error) { console.error(data.error); break; }
                    if (!isPresenting) break; 

                    // B. Reveal & Speak
                    await renderPage(currentSlide);
                    statusEl.textContent = `Slide ${currentSlide} of ${totalSlides}`;

                    if (data.text) {
                        addLog("Elsa", data.text);
                        if (data.audio) await playAudioData(data.audio);
                    }

                    // C. Logic Checkpoint (Only if NOT interrupted by VAD during speech)
                    if (isPresenting && !isInterrupted && currentSlide < totalSlides) {
                         const shouldAsk = await shouldAskQuestion(text, currentSlide, totalSlides, slidesSinceLastAsk, recentUserQuestion);
                         
                         // Double check interrupt before asking
                         if (!isInterrupted && shouldAsk) {
                            const checkText = "Does that make sense?";
                            const ttsRes = await fetch(`${BASE_URL}/agent/speak`, {
                                method: 'POST',
                                headers: {'Content-Type': 'application/json'},
                                body: JSON.stringify({ text: checkText, type: 'TTS_ONLY' })
                            });
                            const ttsData = await ttsRes.json();
                            await playAudioData(ttsData.audio);

                            // Force listen state
                            interruptAndListen();
                            // Wait loop handled by main loop check
                         } else {
                            slidesSinceLastAsk += 1;
                         }
                    }

                    if (isPresenting && !isInterrupted) currentSlide++; 

                } catch (err) {
                    console.error(err);
                    break;
                }
            }
        };


        // --- HELPERS ---
        async function playAudioData(base64Audio) {
            const binaryString = window.atob(base64Audio);
            const bytes = new Uint8Array(binaryString.length);
            for (let i = 0; i < binaryString.length; i++) bytes[i] = binaryString.charCodeAt(i);
            
            simli.sendAudioData(bytes);

            // Wait for audio duration
            // BUT: Check for interruption constantly
            const durationMs = (bytes.length / 2 / 16000) * 1000;
            const steps = 20; 
            const stepTime = (durationMs + 200) / steps;

            for(let i=0; i<steps; i++) {
                if (isInterrupted) {
                    simli.ClearBuffer();
                    break; 
                }
                await new Promise(r => setTimeout(r, stepTime));
            }
        }

        async function renderHiddenPage(num) {
            if(num > pdfDoc.numPages) return;
            const page = await pdfDoc.getPage(num);
            const viewport = page.getViewport({ scale: 1.5 });
            const hiddenCanvas = document.getElementById('hidden-canvas');
            const hiddenCtx = hiddenCanvas.getContext('2d');
            hiddenCanvas.height = viewport.height;
            hiddenCanvas.width = viewport.width;
            await page.render({ canvasContext: hiddenCtx, viewport: viewport }).promise;
        }

        async function shouldAskQuestion(slideText, slideIndex, totalSlides, sinceLastAsk, hadRecentUserQuestion) {
            try {
                const res = await fetch(`${BASE_URL}/agent/speak`, {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ text: slideText, type: 'SHOULD_ASK', slideIndex, totalSlides, slidesSinceLastAsk: sinceLastAsk, recentUserQuestion: hadRecentUserQuestion })
                });
                const data = await res.json();
                return !!data.ask;
            } catch { return false; }
        }

        // Standard Helpers
        function addLog(role, text) {
            const div = document.createElement('div');
            div.className = "msg-row";
            div.innerHTML = `<div class="msg-role role-${role.toLowerCase()}">${role}:</div><div class="msg-text">${text}</div>`;
            chatLog.appendChild(div);
            chatLog.scrollTop = chatLog.scrollHeight;
        }

        const canvas = document.getElementById('pdf-canvas');
        const ctx = canvas.getContext('2d');
        
        document.getElementById('pdf-input').onchange = async (e) => {
            const file = e.target.files[0];
            const data = new Uint8Array(await file.arrayBuffer());
            pdfDoc = await pdfjsLib.getDocument({ data }).promise;
            currentSlide = 1;
            renderPage(1);
            if(statusEl.textContent.includes("Online")) enableControls(true);
        };

        async function renderPage(num) {
            if(num > pdfDoc.numPages) return;
            const page = await pdfDoc.getPage(num);
            const viewport = page.getViewport({ scale: 1.5 });
            canvas.height = viewport.height;
            canvas.width = viewport.width;
            await page.render({ canvasContext: ctx, viewport: viewport }).promise;
        }
        
        async function getPageText(num) {
            if(num > pdfDoc.numPages) return "";
            const page = await pdfDoc.getPage(num);
            const c = await page.getTextContent();
            return c.items.map(i => i.str).join(' ');
        }
        
        // --- 5. STOP ---
        stopBtn.onclick = () => {
            isPresenting = false;
            isInterrupted = false;
            simli.ClearBuffer();
            statusEl.textContent = "Stopped";
            startBtn.disabled = false;
            stopBtn.disabled = true;
            micBtn.classList.remove('listening');
        };
    </script>
</body>
</html>
